{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression#\n",
    "\n",
    "**学习内容：**  \n",
    "- 从0搭建Logistic Regression算法：\n",
    "    - 数据的简单预处理\n",
    "    - 网络结构的搭建\n",
    "    - 权值的初始化  \n",
    "    - 损失函数的计算，梯度计算\n",
    "    - 优化方法(梯度下降)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需要调用的库 #\n",
    "\n",
    "- numpy： 常用的科学计算库。\n",
    "- h5py： 用于hdf5文件的读取、操作。\n",
    "- matplotlib： 用于数据可视化。\n",
    "- PIL and scipy： 这里用于测试自己的图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from depends.lr_utils import load_dataset\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.数据预处理#\n",
    "\n",
    "**数据集：**  \n",
    "\n",
    "这里的数据集为带label的图片数据\n",
    "- label = 1代表cat\n",
    "- label = 0代表noncat\n",
    "\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, 分别存储训练集原始图像、训练集labels、测试集原始图像、测试集labels\n",
    "\n",
    "后续会对原始图像进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为方便计算，将每张图片压缩为一个一维向量  \n",
    "\n",
    "**这里用到：**  \n",
    "\n",
    "\n",
    "X.reshape(X.shape[0], -1) \n",
    "\n",
    "对于train_set_x_orig,train_set_x_orig.shape(0)代表样本数,-1代表{总维数/train_set_x_orig.shape(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集原始形状： (209, 64, 64, 3)\n",
      "测试集集原始形状： (50, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"训练集原始形状：\", train_set_x_orig.shape)\n",
    "print(\"测试集集原始形状：\", test_set_x_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集压缩后形状： (12288, 209)\n",
      "测试集压缩后形状： (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "print(\"训练集压缩后形状：\", train_set_x_orig.shape)\n",
    "print(\"测试集压缩后形状：\", test_set_x_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**归一化:**\n",
    "\n",
    "一般来说在训练模型之前需要对数据进行中心化与归一化,处理过程遵循公式：\n",
    "<br><br>\n",
    "<font size = 6>\n",
    "$\\frac{x-μ}{σ}$\n",
    "</font>\n",
    "\n",
    "$μ$是样本均值,$σ$是样本标准差\n",
    "\n",
    "这里只简单对图像像素值进行缩放，让每个像素的值除以255(像素值大小上限)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_orig/255\n",
    "test_set_x = test_set_x_orig/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.网络搭建 #\n",
    "\n",
    "我们搭建一个最简单的不含隐层的Neural Network,基本结构如下图：\n",
    "<img src = \"source/simple_nn.jpg\"></img>\n",
    "\n",
    "左边一列向量代表一个压缩后的样本  \n",
    "每个小圆圈是一个输入层神经元，负责接收样本的一个像素数据  \n",
    "每个输入神经元对输出神经元进行一个线性连接  \n",
    "用sigmoid函数对输出进行激励  \n",
    "用logloss计算损失  \n",
    "最小化代价函数  \n",
    "\n",
    "**用数学公式解释：**  \n",
    "对每个样本 <font size = 4>$x^{(i)}$:  \n",
    "\n",
    "$z^{(i)} = w^T x^{(i)} + b$  \n",
    "$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$  \n",
    "$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$ </font>  \n",
    "\n",
    "最终的代价函数为:\n",
    "<font size = 4>$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$</font>   \n",
    "而我们的目标就是最小化代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 我们通过以下步骤搭建网络#\n",
    "1.定义网络各部分组件(Sigmoid函数、logloss等等)  \n",
    "2.初始化网络权重  \n",
    "3.迭代训练:\n",
    "- 计算损失 (前向传播)\n",
    "- 计算梯度 (后向传播)\n",
    "- 更新权重 (梯度下降)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.定义Sigmoid函数#\n",
    "\n",
    "<font size = 4>$Sigmoid = \\frac{1}{1 - e^-x}$<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    result = 1 / (1 + np.exp(-x))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.权值初始化#\n",
    "这里利用np.zero(dim)初始化w,dim为维度  \n",
    "直接用0初始化b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_params(dims):\n",
    "    w = np.zeros((dims, 1))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.前后向传播 #\n",
    "- 前向传播即是求cost的过程\n",
    "- 后向传播即是求偏导   \n",
    "\n",
    "<font size = 5>\n",
    "$\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$\n",
    "<br><br>\n",
    "$\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    A = Sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(1.0 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    cost = np.squeeze(cost)\n",
    "    dw = (1.0 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1.0 / m) * np.sum(A - Y) \n",
    "    return cost, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.迭代训练优化#\n",
    "对于参数$\\theta$, 更新规则是： $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, $\\alpha$ 是指学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    for i in range(num_iterations):\n",
    "        cost, dw, db = propagate(w, b, X, Y)\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.将训练好的w与b用于测试 #\n",
    "\n",
    "最终的输出是一个概率,这里设概率大于0.5为cat,反之为noncat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b ,X):\n",
    "    m = X.shape[1]\n",
    "    A = Sigmoid(np.dot(w.T, X) + b)\n",
    "    mask = A > 0.5\n",
    "    Y = np.ones_like(A)\n",
    "    Y = Y * mask\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.组件集成 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = False):\n",
    "    dim = X_train.shape[0]\n",
    "    w, b = init_params(dim)\n",
    "    _w, _b = optimize(w, b, X_train, Y_train, num_iterations = num_iterations, learning_rate = learning_rate, print_cost = print_cost)\n",
    "    pred_train = predict(_w, _b, X_train)\n",
    "    pred_test = predict(_w, _b, X_test)\n",
    "    accuracy_train = 1 -  np.mean(np.abs(pred_train - Y_train))\n",
    "    accurary_test = 1- np.mean(np.abs(pred_test - Y_test))\n",
    "    print(\"accuracy in train set = \", accuracy_train)\n",
    "    print(\"accuracy in test set = \", accurary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "accuracy in train set =  0.99043062201\n",
      "accuracy in test set =  0.7\n"
     ]
    }
   ],
   "source": [
    "model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
