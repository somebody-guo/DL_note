{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化方法 #\n",
    "梯度下降是最经典的优化方法，但随着数据量的加大，梯度下降的难度也在增加  \n",
    "除了梯度下降之外，还有数种优化的梯度下降方法，它们能让你在更短的时间内获得更好的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "from depends.opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
    "from depends.opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
    "from depends.testCases_opt import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降(Gradient Descent) #\n",
    "GD是最经典的优化方法，它每次使用所有数据来进行更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" +  str(l+1)] \n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" +  str(l+1)] \n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.63535156 -0.62320365 -0.53718766]\n",
      " [-1.07799357  0.85639907 -2.29470142]]\n",
      "b1 = [[ 1.74604067]\n",
      " [-0.75184921]]\n",
      "W2 = [[ 0.32171798 -0.25467393  1.46902454]\n",
      " [-2.05617317 -0.31554548 -0.3756023 ]\n",
      " [ 1.1404819  -1.09976462 -0.1612551 ]]\n",
      "b2 = [[-0.88020257]\n",
      " [ 0.02561572]\n",
      " [ 0.57539477]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
    "\n",
    "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降(Stochastic GD)与mini-batch梯度下降(mini-batch GD)\n",
    "在数据量增大，单个数据内容提高的同时，GD的负担会越来越重，因此提出了SGD与mini-batch GD  \n",
    "代码如下：  \n",
    "- **(Batch) Gradient Descent**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost.\n",
    "    cost = compute_cost(a, Y)\n",
    "    # Backward propagation.\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "- **Stochastic Gradient Descent**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost = compute_cost(a, Y[:,j])\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD每次随机抽取一个样本进行迭代更新，这样对硬件的容量要求就很小，并且每一次更新都很快  \n",
    "但是由于一个样本的分布可能和整体分布有较大差距，导致更新时不一定朝着“下降”的方向，而引起震荡    \n",
    "下图是SGD与GD的更新趋势对比图：  \n",
    "<img src = \"source\\SGD vs GD.png\"></img>  \n",
    "\n",
    "mini-batch GD是GD与SGD的平衡，每次取一批数据进行更新，这样使震荡减小，而又不至于消耗太多硬件资源   \n",
    "下图是SGD与mini-batch的更新趋势对比图：  \n",
    "<img src = \"source\\SGD vs mini-batch GD.png\"></img>  \n",
    "\n",
    "### mini-batch GD的实现#\n",
    "分两个步骤：  \n",
    "- 混淆：打乱训练集，注意打乱后的X与Y要相互对应\n",
    "- 偏移：每次根据一个便宜量，从打乱得数据集中抽取一批(a batch)的数据用于训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "\n",
    "    np.random.seed(seed)            \n",
    "    m = X.shape[1]                  \n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size: (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size: (k + 1) * mini_batch_size]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size: ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size: ]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (12288, 64)\n",
      "shape of the 2nd mini_batch_X: (12288, 64)\n",
      "shape of the 3rd mini_batch_X: (12288, 20)\n",
      "shape of the 1st mini_batch_Y: (1, 64)\n",
      "shape of the 2nd mini_batch_Y: (1, 64)\n",
      "shape of the 3rd mini_batch_Y: (1, 20)\n",
      "mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n"
     ]
    }
   ],
   "source": [
    "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
    "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量(Momentum)\n",
    "Momentum的提出是为了改善mini-batch GD的震荡问题  \n",
    "Momentum通过移动加权平均，累计一个历史更新，如果本次更新方向同历史方向，则会加速更新，否则会抑制更新，表现为抑制震荡  \n",
    "<img src = \"source\\Momentum.png\"></img>\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}$$  \n",
    "\n",
    "$\\alpha$是学习率,$\\beta$是动量的权重，一般设为0.9  \n",
    "首先要对动量V(历史更新)进行初始化，如果初始化为零，需要经过一定的迭代才能生效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v[\"dW1\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "v[\"db1\"] = [[ 0.]\n",
      " [ 0.]]\n",
      "v[\"dW2\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "v[\"db2\"] = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_velocity_test_case()\n",
    "\n",
    "v = initialize_velocity(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grads[\"db\" + str(l+1)]\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * v[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * v[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.62544598 -0.61290114 -0.52907334]\n",
      " [-1.07347112  0.86450677 -2.30085497]]\n",
      "b1 = [[ 1.74493465]\n",
      " [-0.76027113]]\n",
      "W2 = [[ 0.31930698 -0.24990073  1.4627996 ]\n",
      " [-2.05974396 -0.32173003 -0.38320915]\n",
      " [ 1.13444069 -1.0998786  -0.1713109 ]]\n",
      "b2 = [[-0.87809283]\n",
      " [ 0.04055394]\n",
      " [ 0.58207317]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[ 0.02344157]\n",
      " [ 0.16598022]\n",
      " [ 0.07420442]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
    "\n",
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "Adam是Momentum与RMSprop的结合,一方面利用Momentum来抑制震荡，一方面利用RMSprop来自动调节学习率  \n",
    "**算法步骤：**\n",
    "- 计算Momentum项：\n",
    "\n",
    "$$v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} }$$\n",
    "- 计算Momentum的修正(改善算法开始运行时，历史更新量的累计需要一定迭代的问题)：\n",
    "\n",
    "$$v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t}$$\n",
    "- 计算RMSprop项：\n",
    "\n",
    "$$s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2$$\n",
    "- 计算RMSprop的修正：\n",
    "\n",
    "$$s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t}$$\n",
    "- 权重更新：\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v[\"dW1\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "v[\"db1\"] = [[ 0.]\n",
      " [ 0.]]\n",
      "v[\"dW2\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "v[\"db2\"] = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "s[\"dW1\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "s[\"db1\"] = [[ 0.]\n",
      " [ 0.]]\n",
      "s[\"dW2\"] = [[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "s[\"db2\"] = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_adam_test_case()\n",
    "\n",
    "v, s = initialize_adam(parameters)\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                        \n",
    "    s_corrected = {}                        \n",
    "\n",
    "    for l in range(L):\n",
    "\n",
    "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)]\n",
    "\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - np.power(beta1, t))\n",
    "\n",
    "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * np.square(grads[\"dW\" + str(l+1)])\n",
    "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * np.square(grads[\"db\" + str(l+1)])\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - np.power(beta2, t))\n",
    "\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate *  v_corrected[\"dW\" + str(l+1)] / (np.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon)\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate *  v_corrected[\"db\" + str(l+1)] / (np.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon)\n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.63178673 -0.61919778 -0.53561312]\n",
      " [-1.08040999  0.85796626 -2.29409733]]\n",
      "b1 = [[ 1.75225313]\n",
      " [-0.75376553]]\n",
      "W2 = [[ 0.32648046 -0.25681174  1.46954931]\n",
      " [-2.05269934 -0.31497584 -0.37661299]\n",
      " [ 1.14121081 -1.09244991 -0.16498684]]\n",
      "b2 = [[-0.88529979]\n",
      " [ 0.03477238]\n",
      " [ 0.57537385]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[ 0.02344157]\n",
      " [ 0.16598022]\n",
      " [ 0.07420442]]\n",
      "s[\"dW1\"] = [[ 0.00121136  0.00131039  0.00081287]\n",
      " [ 0.0002525   0.00081154  0.00046748]]\n",
      "s[\"db1\"] = [[  1.51020075e-05]\n",
      " [  8.75664434e-04]]\n",
      "s[\"dW2\"] = [[  7.17640232e-05   2.81276921e-04   4.78394595e-04]\n",
      " [  1.57413361e-04   4.72206320e-04   7.14372576e-04]\n",
      " [  4.50571368e-04   1.60392066e-07   1.24838242e-03]]\n",
      "s[\"db2\"] = [[  5.49507194e-05]\n",
      " [  2.75494327e-03]\n",
      " [  5.50629536e-04]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
    "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
